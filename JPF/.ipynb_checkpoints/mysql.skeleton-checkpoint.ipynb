{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tables\n",
    "\n",
    "TO PREPARE:\n",
    " - The .csv files referenced below are STRAIGHT exports from the Socrata Files. \n",
    " - Each Socrata file is linked below. \n",
    "\n",
    " - The 311 data .csv import implies that you downloaded a monolithic 311 file from Socrata and then ran bash \"split\" on the file. \n",
    " - Once the split files were obtained, you then further processed them to `cat columns.311 [split_file] > [split_file]_c` to obtain properly \"headered\" split files. \n",
    "\n",
    "TO DO: \n",
    " - Please note that the guess_sql code above makes absurdly large varchar fields to account for large description fields in some data tables (specifically HPD Violations NOVDescription)\n",
    " - Need to clean up the field names for 311\n",
    "\n",
    "NOTES: \n",
    " - Far below is some random SQL SELECT statements\n",
    " - Far below are SQL statements for creating table indices\n",
    " - Questions: jpf321@gmail.com slack: jpfreeley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import desired libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:20:23.556059",
     "start_time": "2016-11-20T15:20:23.513018"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will log to /Users/jfreeley/Desktop/HeatSeek/db_import.log\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "BASE_DIR = '/Users/jfreeley/Desktop/HeatSeek/'\n",
    "\n",
    "LOG_FILE = BASE_DIR+'db_import.log'\n",
    "\n",
    "logging.basicConfig(format= '[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    filename=LOG_FILE, \n",
    "    level=logging.INFO)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "print \"This notebook will log to {}\".format(LOG_FILE)\n",
    "log.info(\"This notebook will log to {}\".format(LOG_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T13:40:58.696220",
     "start_time": "2016-11-16T13:40:58.691108"
    }
   },
   "source": [
    "# Initialize connection to AWS mySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:20:26.139850",
     "start_time": "2016-11-20T15:20:26.098134"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AWS\n",
    "#engine = create_engine('mysql+mysqlconnector://hsdbuser:hsdbpass@hsdb.cjjva3uq32na.us-west-2.rds.amazonaws.com:3306/heatseek', echo=False)\n",
    "\n",
    "### LOCALHOST\n",
    "### INSTALL ON MAC\n",
    "### brew update\n",
    "### brew doctor\n",
    "### brew upgrade\n",
    "### brew install mysql\n",
    "### brew services start mysql\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root@localhost/heatseek', echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:20:29.363872",
     "start_time": "2016-11-20T15:20:29.145524"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_COLUMN_LENGTH = 255\n",
    "\n",
    "\n",
    "def guess_sqlcol(dfparam):    \n",
    "\n",
    "## GUESS AT SQL COLUMN TYPES FROM DataFrame dtypes. \n",
    "    \n",
    "    dtypedict = {}\n",
    "    for i,j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.NVARCHAR(length=MAX_COLUMN_LENGTH)}) ##big field length for HPD violations description\n",
    "\n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.Float(precision=20, asdecimal=True)}) ##big precision for LAT/LONG fields\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.INT()})\n",
    "\n",
    "    return dtypedict\n",
    "\n",
    "\n",
    "def hpd_csv2sql(description, input_csv_file, sep_char, output_pickle,\\\n",
    "            table_name, dtype_dict, parse_dates, load_pickle, \\\n",
    "                input_pickle, db_action, truncate_columns, date_time_columns,\\\n",
    "               chunk_size, keep_cols):\n",
    "\n",
    "    log.info(\"Beginning {} Import {}\".format(description,datetime.datetime.now()))\n",
    "    \n",
    "    if load_pickle == True:\n",
    "        log.info(\"Flagged load of PICKLE: {} = True\".format(input_pickle))\n",
    "        \n",
    "        with open(input_pickle, 'r') as picklefile:\n",
    "            log.info(\"Begin OPEN {} Pickle: {}\".format(input_pickle, datetime.datetime.now()))\n",
    "            log.info(\"Great we have a pickle file...Loading from {}\".format(input_pickle))\n",
    "            df = pickle.load(picklefile)\n",
    "\n",
    "    else: \n",
    "        log.info(\"Reading CSV from {} .. This may take a while...\".format(input_csv_file))\n",
    "        \n",
    "        with open(input_csv_file, 'r') as input_csv: ## should just change to IF EXISTS rather than open()???\n",
    "            df = pd.read_csv(input_csv_file , sep=sep_char, dtype=dtype_dict, parse_dates=parse_dates)\n",
    "                \n",
    "        log.info(\"Why don't we save {} for next time\".format(output_pickle))\n",
    "        \n",
    "        with open(output_pickle, 'w') as picklefile:\n",
    "            log.info(\"Begin {} Pickle: {}\".format(description,datetime.datetime.now()))\n",
    "            pickle.dump(df, picklefile)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## LET'S SEE IF THERE ARE COLUMNS TO TRUNCATE\n",
    "    ## CLEAN COLUMN NAMES\n",
    "    cols = [i.lower().replace(\" \",\"_\").replace(\"'\",\"\").replace(\"\\xe2\\x80\\x99\",\"\").\\\n",
    "            replace(\"#\",\"num\").replace(\"&\",\"and\").replace(\"(\",\"\").\\\n",
    "            replace(\")\",\"\") for i in df.columns]\n",
    "    df.columns = cols\n",
    "    \n",
    "    ## KEEP ONLY THE COLUMNS OF INTEREST\n",
    "    df = df[keep_cols]\n",
    "    \n",
    "    ## TRIM COLUMN DATA TO MAX_LENGTH\n",
    "    for i in truncate_columns:\n",
    "        df[i] = df[i].str[:MAX_COLUMN_LENGTH]\n",
    "    \n",
    "    ## CONVERT DTETIME COLS TO DATETIME\n",
    "    for i in date_time_columns:\n",
    "        df[i] = pd.to_datetime(df[i])\n",
    "\n",
    "        \n",
    "    log.info(\"Let's now try to send it to the DB\")\n",
    "    outputdict = guess_sqlcol(df)  #Guess at SQL columns based on DF dtypes\n",
    "\n",
    "    log.info(\"Begin Upload {} SQL\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Let's see if we should replace or append our table ...\")\n",
    "\n",
    "    if db_action == 'replace': \n",
    "        \n",
    "        action = db_action \n",
    "\n",
    "    else:\n",
    "        \n",
    "        action = 'append'\n",
    "    \n",
    "    log.info(\"We're going with db_action = {}\".format(action))\n",
    "    log.info(\"Sending our df to {}\".format(table_name))\n",
    "    df.to_sql(name=table_name, con=engine, if_exists = action,\\\n",
    "              index=False, chunksize=chunk_size, dtype = outputdict)\n",
    "\n",
    "    log.info(\"Completed {} Import\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Imported: {} rows\".format(df.shape[0]))\n",
    "\n",
    "#%load_ext sql\n",
    "#%sql postgresql://jfreeley@localhost:5432/inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Violations\n",
    "https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Violations/wvxf-dwi5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T18:31:28.126527",
     "start_time": "2016-11-20T18:11:00.463853"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vio_dtype_dict = {\n",
    "'ViolationID':                'int64',\n",
    "'BuildingID':                 'int64',\n",
    "'RegistrationID':             'int64',\n",
    "'BoroID':                     'int64',\n",
    "'Boro':                      'object',\n",
    "'HouseNumber':               'object',\n",
    "'LowHouseNumber':            'object',\n",
    "'HighHouseNumber':           'object',\n",
    "'StreetName':                'object',\n",
    "'StreetCode':                 'int64',\n",
    "'Zip':                      'float64',\n",
    "'Apartment':                 'object',\n",
    "'Story':                     'object',\n",
    "'Block':                      'int64',\n",
    "'Lot':                        'int64',\n",
    "'Class':                     'object',\n",
    "'InspectionDate':            'object',\n",
    "'ApprovedDate':              'object',\n",
    "'OriginalCertifyByDate':     'object',\n",
    "'OriginalCorrectByDate':     'object',\n",
    "'NewCertifyByDate':          'object',\n",
    "'NewCorrectByDate':          'object',\n",
    "'CertifiedDate':             'object',\n",
    "'OrderNumber':               'object',\n",
    "'NOVID':                    'float64',\n",
    "'NOVDescription':            'object',\n",
    "'NOVIssuedDate':             'object',\n",
    "'CurrentStatusID':            'int64',\n",
    "'CurrentStatus':             'object',\n",
    "'CurrentStatusDate':         'object'\n",
    "}    \n",
    "\n",
    "vio_parse_dates = ['InspectionDate',\n",
    "'ApprovedDate',\n",
    "'OriginalCertifyByDate',\n",
    "'OriginalCorrectByDate',\n",
    "'NewCertifyByDate',\n",
    "'NewCorrectByDate',\n",
    "'CertifiedDate',\n",
    "'NOVIssuedDate',\n",
    "'CurrentStatusDate'] \n",
    "\n",
    "vio_date_time_columns = ['inspectiondate',\n",
    "'approveddate',\n",
    "'originalcertifybydate',\n",
    "'originalcorrectbydate',\n",
    "'newcertifybydate',\n",
    "'newcorrectbydate',\n",
    "'certifieddate',\n",
    "'novissueddate',\n",
    "'currentstatusdate'] \n",
    "    \n",
    "vio_df_keep_cols = [\n",
    "    'violationid',\n",
    "    'buildingid',\n",
    "    'registrationid',\n",
    "    'boroid',\n",
    "    'boro',\n",
    "    'housenumber',\n",
    "    'lowhousenumber',\n",
    "    'highhousenumber',\n",
    "    'streetname',\n",
    "    'streetcode',\n",
    "    'zip',\n",
    "    'apartment',\n",
    "    'story',\n",
    "    'block',\n",
    "    'lot',\n",
    "    'class',\n",
    "    'inspectiondate',\n",
    "    'approveddate',\n",
    "    'originalcertifybydate',\n",
    "    'originalcorrectbydate',\n",
    "    'newcertifybydate',\n",
    "    'newcorrectbydate',\n",
    "    'certifieddate',\n",
    "    'ordernumber',\n",
    "    'novid',\n",
    "    'novdescription',\n",
    "    'novissueddate',\n",
    "    'currentstatusid',\n",
    "    'currentstatus',\n",
    "    'currentstatusdate'\n",
    "]\n",
    "vio_description = \"HPD Violations\"\n",
    "vio_input_csv_file = BASE_DIR+'HPD/Data Files/Violations/Housing_Maintenance_Code_Violations.csv'\n",
    "vio_sep_char = \",\"\n",
    "vio_output_pickle = BASE_DIR+'HPD/Data Files/Violations/df_violations.pkl'\n",
    "vio_table_name = 'hpd_violations'\n",
    "vio_load_pickle = True\n",
    "vio_input_pickle = BASE_DIR+'HPD/Data Files/Violations/df_violations.pkl'\n",
    "vio_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "vio_truncate_columns = ['novdescription']\n",
    "vio_chunk_size = 5000\n",
    "\n",
    "hpd_csv2sql(\n",
    "            vio_description,\n",
    "            vio_input_csv_file, \n",
    "            vio_sep_char,\n",
    "            vio_output_pickle, \n",
    "            vio_table_name, \n",
    "            vio_dtype_dict, \n",
    "            vio_parse_dates,\n",
    "            vio_load_pickle,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            vio_input_pickle,\n",
    "            vio_db_action, # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "            vio_truncate_columns, \n",
    "            vio_date_time_columns, \n",
    "            vio_chunk_size,\n",
    "            vio_df_keep_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Buildings \n",
    "https://data.cityofnewyork.us/Housing-Development/Buildings-Subject-to-HPD-Jurisdiction/kj4p-ruqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:24:29.388667",
     "start_time": "2016-11-20T15:21:29.360461"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bld_dtype_dict = {\n",
    "'BuildingID':              'int64',\n",
    "'BoroID':                  'int64',\n",
    "'Boro':                   'object',\n",
    "'HouseNumber':            'object',\n",
    "'LowHouseNumber':         'object',\n",
    "'HighHouseNumber':        'object',\n",
    "'StreetName':             'object',\n",
    "'Zip':                    'object',\n",
    "'Block':                   'int64',\n",
    "'Lot':                     'int64',\n",
    "'BIN':                   'float64',\n",
    "'CommunityBoard':          'int64',\n",
    "'CensusTract':           'float64',\n",
    "'ManagementProgram':      'object',\n",
    "'DoBBuildingClassID':    'float64',\n",
    "'DoBBuildingClass':       'object',\n",
    "'LegalStories':          'float64',\n",
    "'LegalClassA':           'float64',\n",
    "'LegalClassB':           'float64',\n",
    "'RegistrationID':          'int64',\n",
    "'LifeCycle':              'object',\n",
    "'RecordStatusID':          'int64',\n",
    "'RecordStatus':           'object'\n",
    "}\n",
    "\n",
    "bld_df_keep_cols = [\n",
    "    'buildingid',\n",
    "    'boroid',\n",
    "    'boro',\n",
    "    'housenumber',\n",
    "    'lowhousenumber',\n",
    "    'highhousenumber',\n",
    "    'streetname',\n",
    "    'zip',\n",
    "    'block',\n",
    "    'lot',\n",
    "    'bin',\n",
    "    'communityboard',\n",
    "    'censustract',\n",
    "    'managementprogram',\n",
    "    'dobbuildingclassid',\n",
    "    'dobbuildingclass',\n",
    "    'legalstories',\n",
    "    'legalclassa',\n",
    "    'legalclassb',\n",
    "    'registrationid',\n",
    "    'lifecycle',\n",
    "    'recordstatusid',\n",
    "    'recordstatus'\n",
    "]\n",
    "\n",
    "bld_parse_dates = None\n",
    "\n",
    "bld_description = \"HPD Buildings\"\n",
    "bld_input_csv_file = BASE_DIR+'HPD/Data Files/Buildings/Buildings_Subject_to_HPD_Jurisdiction.csv'\n",
    "bld_sep_char = \",\"\n",
    "bld_output_pickle = BASE_DIR+'HPD/Data Files/Buildings/df_buildings.pkl'\n",
    "bld_table_name = 'hpd_buildings'\n",
    "bld_load_pickle = True\n",
    "bld_input_pickle = BASE_DIR+'HPD/Data Files/Buildings/df_buildings.pkl'\n",
    "bld_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "bld_truncate_columns = ''\n",
    "bld_date_time_columns = ''\n",
    "bld_chunk_size = 5000\n",
    "\n",
    "\n",
    "hpd_csv2sql(\n",
    "            bld_description,\n",
    "            bld_input_csv_file, \n",
    "            bld_sep_char,\n",
    "            bld_output_pickle, \n",
    "            bld_table_name, \n",
    "            bld_dtype_dict, \n",
    "            bld_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            bld_input_pickle,\n",
    "            'replace', # DB ACTiON set as REPLACE (rather than APPEND),\n",
    "            bld_truncate_columns, \n",
    "            bld_date_time_columns, \n",
    "            bld_chunk_size,\n",
    "            bld_df_keep_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Complaints\n",
    "https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Complaints/uwyv-629c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T15:48:58.781092",
     "start_time": "2016-11-20T15:44:52.723488"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmp_dtype_dict = {\n",
    "'ComplaintID':         'int64',\n",
    "'BuildingID':          'int64',\n",
    "'BoroughID':           'int64',\n",
    "'Borough':            'object',\n",
    "'HouseNumber':        'object',\n",
    "'StreetName':         'object',\n",
    "'Zip':               'float64',\n",
    "'Block':               'int64',\n",
    "'Lot':                 'int64',\n",
    "'Apartment':          'object',\n",
    "'CommunityBoard':      'int64',\n",
    "'ReceivedDate':       'object',\n",
    "'StatusID':            'int64',\n",
    "'Status':             'object',\n",
    "'StatusDate':         'object'\n",
    "}\n",
    "\n",
    "cmp_df_keep_cols = [\n",
    "    'complaintid',\n",
    "    'buildingid',\n",
    "    'boroughid',\n",
    "    'borough',\n",
    "    'housenumber',\n",
    "    'streetname',\n",
    "    'zip',\n",
    "    'block',\n",
    "    'lot',\n",
    "    'apartment',\n",
    "    'communityboard',\n",
    "    'receiveddate',\n",
    "    'statusid',\n",
    "    'status',\n",
    "    'statusdate'\n",
    "]\n",
    "\n",
    "cmp_parse_dates = ['StatusDate','ReceivedDate']\n",
    "cmp_date_time_columns = ['statusdate','receiveddate']\n",
    "\n",
    "cmp_truncate_columns = ''\n",
    "\n",
    "cmp_description = \"HPD Complaints\"\n",
    "cmp_input_csv_file = BASE_DIR+'HPD/Data Files/Complaints/Housing_Maintenance_Code_Complaints.csv'\n",
    "cmp_sep_char = \",\"\n",
    "cmp_output_pickle = BASE_DIR+'HPD/Data Files/Complaints/df_complaints.pkl'\n",
    "cmp_table_name = 'hpd_complaints'\n",
    "cmp_load_pickle = True\n",
    "cmp_input_pickle = BASE_DIR+'HPD/Data Files/Complaints/df_complaints.pkl'\n",
    "cmp_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "cmp_chunk_size = 5000\n",
    "\n",
    "hpd_csv2sql(\n",
    "            cmp_description,\n",
    "            cmp_input_csv_file, \n",
    "            cmp_sep_char,\n",
    "            cmp_output_pickle, \n",
    "            cmp_table_name, \n",
    "            cmp_dtype_dict, \n",
    "            cmp_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            cmp_input_pickle,\n",
    "            'replace', # DB ACTiON set as REPLACE (rather than APPEND),\n",
    "            cmp_truncate_columns, \n",
    "            cmp_date_time_columns, \n",
    "            cmp_chunk_size,\n",
    "            cmp_df_keep_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Complaint - Problems\n",
    "https://data.cityofnewyork.us/Housing-Development/Complaint-Problems/a2nx-4u46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T16:02:56.221538",
     "start_time": "2016-11-20T15:52:20.774594"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpb_dtype_dict = {\n",
    "'ProblemID':             'int64',\n",
    "'ComplaintID':           'int64',\n",
    "'UnitTypeID':            'int64',\n",
    "'UnitType':             'object',\n",
    "'SpaceTypeID':           'int64',\n",
    "'SpaceType':            'object',\n",
    "'TypeID':                'int64',\n",
    "'Type':                 'object',\n",
    "'MajorCategoryID':       'int64',\n",
    "'MajorCategory':        'object',\n",
    "'MinorCategoryID':       'int64',\n",
    "'MinorCategory':        'object',\n",
    "'CodeID':                'int64',\n",
    "'Code':                 'object',\n",
    "'StatusID':              'int64',\n",
    "'Status':               'object',\n",
    "'StatusDate':           'object',\n",
    "'StatusDescription':    'object',\n",
    "}\n",
    "cpb_parse_dates = ['StatusDate']\n",
    "cpb_date_time_columns = ['statusdate']\n",
    "\n",
    "cpb_df_keep_cols = [\n",
    "    'problemid',\n",
    "    'complaintid',\n",
    "    'unittypeid',\n",
    "    'unittype',\n",
    "    'spacetypeid',\n",
    "    'spacetype',\n",
    "    'typeid',\n",
    "    'type',\n",
    "    'majorcategoryid',\n",
    "    'majorcategory',\n",
    "    'minorcategoryid',\n",
    "    'minorcategory',\n",
    "    'codeid',\n",
    "    'code',\n",
    "    'statusid',\n",
    "    'status',\n",
    "    'statusdate',\n",
    "    'statusdescription'\n",
    "]\n",
    "\n",
    "\n",
    "cpb_description = \"HPD ComplaintProblems\"\n",
    "cpb_input_csv_file = BASE_DIR+'HPD/Data Files/Complaints/Complaint_Problems.csv'\n",
    "cpb_sep_char = \",\"\n",
    "cpb_output_pickle = BASE_DIR+'HPD/Data Files/Complaints/df_prob.pkl'\n",
    "cpb_table_name = 'hpd_complaintsProb'\n",
    "cpb_load_pickle = True\n",
    "cpb_input_pickle = BASE_DIR+'HPD/Data Files/Complaints/df_prob.pkl'\n",
    "cpb_db_action = 'replace', ## if not = 'replace' then 'append' \n",
    "cpb_chunk_size = 5000\n",
    "cpb_truncate_columns = ['statusdescription']\n",
    "\n",
    "hpd_csv2sql(\n",
    "            cpb_description,\n",
    "            cpb_input_csv_file, \n",
    "            cpb_sep_char,\n",
    "            cpb_output_pickle, \n",
    "            cpb_table_name, \n",
    "            cpb_dtype_dict, \n",
    "            cpb_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            cpb_input_pickle,\n",
    "            'replace', # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "            cpb_truncate_columns, \n",
    "            cpb_date_time_columns, \n",
    "            cpb_chunk_size,\n",
    "            cpb_df_keep_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registrations\n",
    "https://data.cityofnewyork.us/Housing-Development/Multiple-Dwelling-Registrations/tesw-yqqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T16:06:25.327023",
     "start_time": "2016-11-20T16:05:19.578152"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_dtype_dict = {\n",
    "'RegistrationID':            'int64',\n",
    "'BuildingID':                'int64',\n",
    "'BoroID':                    'int64',\n",
    "'Boro':                     'object',\n",
    "'HouseNumber':              'object',\n",
    "'LowHouseNumber':           'object',\n",
    "'HighHouseNumber':          'object',\n",
    "'StreetName':               'object',\n",
    "'StreetCode':               'int64',\n",
    "'Zip':                     'float64',\n",
    "'Block':                     'int64',\n",
    "'Lot':                       'int64',\n",
    "'BIN':                     'float64',\n",
    "'CommunityBoard':            'int64',\n",
    "'LastRegistrationDate':     'object',\n",
    "'RegistrationEndDate':      'object'}\n",
    "\n",
    "reg_df_keep_cols = [\n",
    "    'registrationid',\n",
    "    'buildingid',\n",
    "    'boroid',\n",
    "    'boro',\n",
    "    'housenumber',\n",
    "    'lowhousenumber',\n",
    "    'highhousenumber',\n",
    "    'streetname',\n",
    "    'streetcode',\n",
    "    'zip',\n",
    "    'block',\n",
    "    'lot',\n",
    "    'bin',\n",
    "    'communityboard',\n",
    "    'lastregistrationdate',\n",
    "    'registrationenddate'\n",
    "]\n",
    "\n",
    "reg_parse_dates = ['LastRegistrationDate', 'RegistrationEndDate']\n",
    "reg_date_time_columns = ['lastregistrationdate', 'registrationenddate']\n",
    "reg_truncate_columns = ''\n",
    "\n",
    "reg_description = \"HPD Registrations\"\n",
    "reg_input_csv_file = BASE_DIR+'HPD/Data Files/Registrations/Multiple_Dwelling_Registrations.csv'\n",
    "reg_sep_char = \",\"\n",
    "reg_output_pickle = BASE_DIR+'HPD/Data Files/Registrations/df_reg.pkl'\n",
    "reg_table_name = 'hpd_registrations'\n",
    "reg_load_pickle = True\n",
    "reg_input_pickle = BASE_DIR+'HPD/Data Files/Registrations/df_reg.pkl'\n",
    "reg_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "reg_chunk_size = 5000\n",
    "\n",
    "hpd_csv2sql(\n",
    "            reg_description,\n",
    "            reg_input_csv_file, \n",
    "            reg_sep_char,\n",
    "            reg_output_pickle, \n",
    "            reg_table_name, \n",
    "            reg_dtype_dict, \n",
    "            reg_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            reg_input_pickle,\n",
    "            'replace', # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "            reg_truncate_columns, \n",
    "            reg_date_time_columns, \n",
    "            reg_chunk_size,\n",
    "            reg_df_keep_cols\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration Contacts\n",
    "https://data.cityofnewyork.us/Housing-Development/Registration-Contacts/feu5-w2e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T16:11:00.274566",
     "start_time": "2016-11-20T16:06:25.329499"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcn_dtype_dict = {\n",
    "'RegistrationContactID':     'int64',\n",
    "'RegistrationID':            'int64',\n",
    "'Type':                     'object',\n",
    "'ContactDescription':       'object',\n",
    "'CorporationName':          'object',\n",
    "'Title':                    'object',\n",
    "'FirstName':                'object',\n",
    "'MiddleInitial':            'object',\n",
    "'LastName':                 'object',\n",
    "'BusinessHouseNumber':      'object',\n",
    "'BusinessStreetName':       'object',\n",
    "'BusinessApartment':        'object',\n",
    "'BusinessCity':             'object',\n",
    "'BusinessState':            'object',\n",
    "'BusinessZip':              'object'\n",
    "    }\n",
    "\n",
    "rcn_df_keep_cols = [\n",
    "    'registrationcontactid',\n",
    "    'registrationid',\n",
    "    'type',\n",
    "    'contactdescription',\n",
    "    'corporationname',\n",
    "    'title',\n",
    "    'firstname',\n",
    "    'middleinitial',\n",
    "    'lastname',\n",
    "    'businesshousenumber',\n",
    "    'businessstreetname',\n",
    "    'businessapartment',\n",
    "    'businesscity',\n",
    "    'businessstate',\n",
    "    'businesszip'\n",
    "]\n",
    "\n",
    "rcn_truncate_columns = ''\n",
    "\n",
    "rcn_parse_dates = ''\n",
    "rcn_date_time_columns = ''\n",
    "\n",
    "rcn_description = \"HPD RegistrationsContacts\"\n",
    "rcn_input_csv_file = BASE_DIR+'HPD/Data Files/Registrations/Registration_Contacts.csv'\n",
    "rcn_sep_char = \",\"\n",
    "rcn_output_pickle = BASE_DIR+'HPD/Data Files/Registrations/df_regCon.pkl'\n",
    "rcn_table_name = 'hpd_registrationContact'\n",
    "rcn_load_pickle = True\n",
    "rcn_input_pickle = BASE_DIR+'HPD/Data Files/Registrations/df_regCon.pkl'\n",
    "rcn_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "rcn_chunk_size = 5000\n",
    "\n",
    "hpd_csv2sql(\n",
    "            rcn_description,\n",
    "            rcn_input_csv_file, \n",
    "            rcn_sep_char,\n",
    "            rcn_output_pickle, \n",
    "            rcn_table_name, \n",
    "            rcn_dtype_dict, \n",
    "            rcn_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            rcn_input_pickle,\n",
    "            'replace', # DB A\n",
    "            rcn_truncate_columns, \n",
    "            rcn_date_time_columns, \n",
    "            rcn_chunk_size,\n",
    "            rcn_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 311 Import\n",
    "https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:16:57.489344",
     "start_time": "2016-11-18T21:16:57.388355"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_311_dtype_dict = {'Unique Key':'int64',\n",
    "'Created Date':'object',\n",
    "'Closed Date':'object',\n",
    "'Agency':'object',\n",
    "'Agency Name':'object',\n",
    "'Complaint Type':'object',\n",
    "'Descriptor':'object',\n",
    "'Location Type':'object',\n",
    "'Incident Zip':'object',\n",
    "'Incident Address':'object',\n",
    "'Street Name':'object',\n",
    "'Cross Street 1':'object',\n",
    "'Cross Street 2':'object',\n",
    "'Intersection Street 1':'object',\n",
    "'Intersection Street 2':'object',\n",
    "'Address Type':'object',\n",
    "'City':'object',\n",
    "'Landmark':'object',\n",
    "'Facility Type':'object',\n",
    "'Status':'object',\n",
    "'Due Date':'object',\n",
    "'Resolution Description':'object',\n",
    "'Resolution Action Updated Date':'object',\n",
    "'Community Board':'object',\n",
    "'Borough':'object',\n",
    "'X Coordinate (State Plane)':'float64',\n",
    "'Y Coordinate (State Plane)':'float64',\n",
    "'Park Facility Name':'object',\n",
    "'Park Borough':'object',\n",
    "'School Name':'object',\n",
    "'School Number':'object',\n",
    "'School Region':'object',\n",
    "'School Code':'object',\n",
    "'School Phone Number':'object',\n",
    "'School Address':'object',\n",
    "'School City':'object',\n",
    "'School State':'object',\n",
    "'School Zip':'object',\n",
    "'School Not Found':'object',\n",
    "'School or Citywide Complaint':'float64',\n",
    "'Vehicle Type':'object',\n",
    "'Taxi Company Borough':'object',\n",
    "'Taxi Pick Up Location':'object',\n",
    "'Bridge Highway Name':'object',\n",
    "'Bridge Highway Direction':'object',\n",
    "'Road Ramp':'object',\n",
    "'Bridge Highway Segment':'object',\n",
    "'Garage Lot Name':'object',\n",
    "'Ferry Direction':'object',\n",
    "'Ferry Terminal Name':'object',\n",
    "'Latitude':'float64',\n",
    "'Longitude':'float64',\n",
    "'Location':'object'}\n",
    "\n",
    "call_311_df_keep_cols = [\n",
    "    \"unique_key\",\n",
    "    \"created_date\",\n",
    "    \"closed_date\",\n",
    "    \"agency\",\n",
    "    \"complaint_type\",\n",
    "    \"descriptor\",\n",
    "    \"incident_zip\",\n",
    "    \"incident_address\",\n",
    "    \"street_name\",\n",
    "    \"cross_street_1\",\n",
    "    \"cross_street_2\",\n",
    "    \"intersection_street_1\",\n",
    "    \"intersection_street_2\",\n",
    "    \"city\",\n",
    "    \"status\",\n",
    "    \"due_date\",\n",
    "    \"resolution_description\",\n",
    "    \"resolution_action_updated_date\",\n",
    "    \"borough\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"location\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: each \"split\" of 250K rows takes about 15min on a macbook air laptop over wifi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T12:26:23.503022",
     "start_time": "2016-11-19T12:19:55.857392"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xaa\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaa_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaa_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaa_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = True\n",
    "call_311_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T13:05:13.437774",
     "start_time": "2016-11-19T12:51:45.319204"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xab\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xab_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xab_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xab_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T13:19:08.010193",
     "start_time": "2016-11-19T13:05:13.440470"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xac\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xac_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xac_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xac_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T13:32:26.846671",
     "start_time": "2016-11-19T13:19:08.013297"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xad\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xad_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xad_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xad_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T13:45:50.512085",
     "start_time": "2016-11-19T13:32:26.849370"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xae\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xae_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xae_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xae_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T13:59:22.723866",
     "start_time": "2016-11-19T13:45:50.515603"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xaf\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaf_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaf_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaf_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T14:12:57.432438",
     "start_time": "2016-11-19T13:59:22.727159"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xag\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xag_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xag_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xag_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T14:26:32.807541",
     "start_time": "2016-11-19T14:12:57.435188"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xah\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xah_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xah_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xah_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T14:30:05.006650",
     "start_time": "2016-11-19T14:26:32.809882"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "call_311_description = \"311_xai\"\n",
    "call_311_input_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xai_c.pkl'\n",
    "call_311_output_pickle = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xai_c.pkl'\n",
    "call_311_input_csv_file = BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xai_c'\n",
    "call_311_sep_char = \",\"\n",
    "call_311_table_name = \"call_311\"\n",
    "call_311_load_pickle = False\n",
    "call_311_db_action = 'append' ## if not = 'replace' then 'append' \n",
    "call_311_truncate_columns = ['Resolution Description']\n",
    "call_311_date_time_columns = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']\n",
    "call_311_chunk_size = 2500\n",
    "\n",
    "hpd_csv2sql(\n",
    "            call_311_description,\n",
    "            call_311_input_csv_file, \n",
    "            call_311_sep_char,\n",
    "            call_311_output_pickle, \n",
    "            call_311_table_name, \n",
    "            call_311_dtype_dict, \n",
    "            call_311_parse_dates,\n",
    "            call_311_load_pickle,   \n",
    "            call_311_input_pickle,\n",
    "            call_311_db_action,\n",
    "            call_311_truncate_columns, \n",
    "            call_311_date_time_columns, \n",
    "            call_311_chunk_size,\n",
    "            call_311_df_keep_cols\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T15:42:35.690566",
     "start_time": "2016-11-16T15:08:38.798811"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xab_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xac_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xad_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xae_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xaf_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL i\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xag_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xah_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + '311/Data Files/2016_Jan1-Nov14/xai_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T22:57:43.075424",
     "start_time": "2016-11-16T22:57:43.049995"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Large 311 Select with ages\n",
    "#\n",
    "# SELECT \n",
    "# `Unique Key`,\n",
    "# `Created Date`, \n",
    "# `Closed Date`,\n",
    "# timestampdiff(day,`Created Date`,`Closed Date`) as AgeDays,\n",
    "# timestampdiff(hour,`Created Date`,`Closed Date`) as AgeHr,\n",
    "# `Agency`,\n",
    "#  `Complaint Type`\n",
    "# ,`Descriptor`,\n",
    "# `Location Type`,\n",
    "#  `Incident Zip`, \n",
    "# `Incident Address`,\n",
    "# `Facility Type`,\n",
    "#  `Status`,\n",
    "# `Due Date`,\n",
    "# `Borough`,\n",
    "#  `Resolution Description`,\n",
    "# `Resolution Action Updated Date`,\n",
    "# `Latitude`,\n",
    "#  `Longitude` \n",
    "# FROM `call_311` \n",
    "# WHERE Agency = \"HPD\" and `Complaint Type` = \"HEAT/HOT WATER\" and `Status` != \"Closed\"\n",
    "\n",
    "\n",
    "\n",
    "# COUNT OF HEAT/HW with locations\n",
    "#\n",
    "# SELECT * \n",
    "# FROM (\n",
    "# SELECT  `Incident Address` ,  `Borough` ,  `Latitude` ,  `Longitude` , COUNT(  `Unique Key` ) AS count, AVG( timestampdiff(\n",
    "# DAY ,  `Created Date` ,  `Closed Date`\n",
    "# ) ) AS average_day_age\n",
    "# FROM call_311\n",
    "# WHERE Agency =  \"HPD\"\n",
    "# AND  `Complaint Type` =  \"HEAT/HOT WATER\"\n",
    "# AND  `Status` =  'Closed'\n",
    "# GROUP BY  `Incident Address`\n",
    "# ) AS count_table\n",
    "# ORDER BY average_day_age DESC\n",
    "\n",
    "\n",
    "#\n",
    "#SELECT TABLE_ROWS, TABLE NAME\n",
    "#      FROM INFORMATION_SCHEMA.TABLES \n",
    "#      WHERE TABLE_SCHEMA = 'heatseak' AND\n",
    "#         TABLE_NAME NOT LIKE '%pma_%';\n",
    "\n",
    "## LIST ALL COLUMNS OF ALL TABLES WITH COL TYPE\n",
    "select table_name, column_name, data_type \n",
    "from information_schema.columns where table_schema = 'heatseek' \n",
    "order by table_name,ordinal_position;\n",
    "\n",
    "## LIST COUNT OF VIOLATION STATUS AND CLASS HPD VIOLATIONS\n",
    "select class, currentstatus, count(violationid) as \n",
    "count from hpd_violations group by currentstatus, class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hpd_buildings indexes\n",
    "ALTER TABLE `hpd_buildings` ADD PRIMARY KEY(`BuildingID`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`BoroID`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`RecordStatus`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`BIN`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`Lot`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`Block`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`StreetName`);\n",
    "ALTER TABLE `hpd_buildings` ADD INDEX(`HouseNumber`);\n",
    "\n",
    "## hpd_complaints indexes\n",
    "ALTER TABLE `hpd_complaints` ADD PRIMARY KEY(`ComplaintID`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`StatusDate`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`Status`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`Lot`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`Block`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`StreetName`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`HouseNumber`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`BoroughID`);\n",
    "ALTER TABLE `hpd_complaints` ADD INDEX(`BuildingID`);\n",
    "\n",
    "## hpd_complaint_problem indexes\n",
    "ALTER TABLE `hpd_complaint_problem` ADD PRIMARY KEY(`ProblemID`);\n",
    "ALTER TABLE `hpd_complaint_problem` ADD INDEX(`ComplaintID`);\n",
    "ALTER TABLE `hpd_complaint_problem` ADD INDEX(`MajorCategory`);\n",
    "ALTER TABLE `hpd_complaint_problem` ADD INDEX(`MinorCategory`);\n",
    "ALTER TABLE `hpd_complaint_problem` ADD INDEX(`Status`);\n",
    "ALTER TABLE `hpd_complaint_problem` ADD INDEX(`StatusDate`);\n",
    "\n",
    "## hpd_registration indexes\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`RegistrationID`); #1062 - Duplicate entry '913236' for key 'PRIMARY'\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`BuildingID`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`BoroID`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`HouseNumber`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`StreetName`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`Block`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`Lot`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`BIN`);\n",
    "ALTER TABLE `hpd_registration` ADD INDEX(`RegistrationEndDate`);\n",
    "\n",
    "## hpd_registrationContact indexes\n",
    "ALTER TABLE `hpd_registrationContact` ADD INDEX(`RegistrationContactID`); ##1062 - Duplicate entry '91323603' for key 'PRIMARY'\n",
    "ALTER TABLE `hpd_registrationContact` ADD INDEX(`RegistrationID`);\n",
    "\n",
    "## call_311 indexes\n",
    "ALTER TABLE `call_311` ADD PRIMARY KEY(`Unique Key`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Created Date`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Agency`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Complaint Type`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Descriptor`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Incident Address`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Status`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Latitude`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Longitude`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Resolution Description`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Resolution Action Updated Date`);\n",
    "ALTER TABLE `call_311` ADD INDEX(`Borough`);\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
