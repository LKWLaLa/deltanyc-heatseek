{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tables\n",
    "\n",
    "TO PREPARE:\n",
    " - The .csv files referenced below are STRAIGHT exports from the Socrata Files. \n",
    " - Each Socrata file is linked below. \n",
    "\n",
    " - The 311 data .csv import implies that you downloaded a monolithic 311 file from Socrata and then ran bash \"split\" on the file. \n",
    " - Once the split files were obtained, you then further processed them to `cat columns.311 [split_file] > [split_file]_c` to obtain properly \"headered\" split files. \n",
    "\n",
    "TO DO: \n",
    " - Please note that the guess_sql code above makes absurdly large varchar fields to account for large description fields in some data tables (specifically HPD Violations NOVDescription)\n",
    " - Need to clean up the field names for 311\n",
    "\n",
    "NOTES: \n",
    " - Far below is some random SQL SELECT statements\n",
    " - Far below are SQL statements for creating table indices\n",
    " - Questions: jpf321@gmail.com slack: jpfreeley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import desired libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T11:58:20.060662",
     "start_time": "2016-11-19T11:58:18.192045"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will log to /Users/jfreeley/Desktop/HeatSeek/db_import.log\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "BASE_DIR = '/Users/jfreeley/Desktop/HeatSeek/'\n",
    "\n",
    "LOG_FILE = BASE_DIR+'db_import.log'\n",
    "\n",
    "logging.basicConfig(format= '[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    filename=LOG_FILE, \n",
    "    level=logging.INFO)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "print \"This notebook will log to {}\".format(LOG_FILE)\n",
    "log.info(\"This notebook will log to {}\".format(LOG_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T13:40:58.696220",
     "start_time": "2016-11-16T13:40:58.691108"
    }
   },
   "source": [
    "# Initialize connection to AWS mySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T11:58:20.119367",
     "start_time": "2016-11-19T11:58:20.063369"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AWS\n",
    "#engine = create_engine('mysql+mysqlconnector://hsdbuser:hsdbpass@hsdb.cjjva3uq32na.us-west-2.rds.amazonaws.com:3306/heatseek', echo=False)\n",
    "\n",
    "### LOCALHOST\n",
    "### INSTALL ON MAC\n",
    "### brew update\n",
    "### brew doctor\n",
    "### brew upgrade\n",
    "### brew install mysql\n",
    "### brew services start mysql\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root@localhost/heatseek', echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T11:58:21.221814",
     "start_time": "2016-11-19T11:58:21.101988"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guess_sqlcol(dfparam):    \n",
    "\n",
    "## GUESS AT SQL COLUMN TYPES FROM DataFrame dtypes. \n",
    "    \n",
    "    dtypedict = {}\n",
    "    for i,j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.NVARCHAR(length=255)})\n",
    "\n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.Float(precision=20, asdecimal=True)}) ##big precision for LAT/LONG fields\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.INT()})\n",
    "\n",
    "    return dtypedict\n",
    "\n",
    "\n",
    "def hpd_csv2sql(description, input_csv_file, sep_char, output_pickle,\\\n",
    "            table_name, dtype_dict, parse_dates, load_pickle, input_pickle, db_action):\n",
    "\n",
    "    log.info(\"Beginning {} Import {}\".format(description,datetime.datetime.now()))\n",
    "    \n",
    "    if load_pickle == True:\n",
    "        log.info(\"Flagged load of PICKLE: {} = True\".format(input_pickle))\n",
    "        \n",
    "        with open(input_pickle, 'r') as picklefile:\n",
    "            log.info(\"Begin OPEN {} Pickle: {}\".format(input_pickle, datetime.datetime.now()))\n",
    "            log.info(\"Great we have a pickle file...Loading from {}\".format(input_pickle))\n",
    "            df = pickle.load(picklefile)\n",
    "\n",
    "    else: \n",
    "        log.info(\"Reading CSV from {} .. This may take a while...\".format(input_csv_file))\n",
    "        \n",
    "        with open(input_csv_file, 'r') as input_csv: ## should just change to IF EXISTS rather than open()???\n",
    "            df = pd.read_csv(input_csv_file , sep=sep_char, dtype=dtype_dict, parse_dates=parse_dates)\n",
    "        \n",
    "        log.info(\"Why don't we save {} for next time\".format(output_pickle))\n",
    "        \n",
    "        with open(output_pickle, 'w') as picklefile:\n",
    "            log.info(\"Begin {} Pickle: {}\".format(description,datetime.datetime.now()))\n",
    "            pickle.dump(df, picklefile)\n",
    "\n",
    "    log.info(\"Let's now try to send it to the DB\")\n",
    "    outputdict = guess_sqlcol(df)  #Guess at SQL columns based on DF dtypes\n",
    "\n",
    "    log.info(\"Begin Upload {} SQL\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Let's see if we should replace or append our table ...\")\n",
    "\n",
    "    if db_action == 'replace': \n",
    "        \n",
    "        action = db_action \n",
    "\n",
    "    else:\n",
    "        \n",
    "        action = 'append'\n",
    "    \n",
    "    log.info(\"We're going with db_action = {}\".format(action))\n",
    "    log.info(\"Sending our df to {}\".format(table_name))\n",
    "    df.to_sql(name=table_name, con=engine, if_exists = action,\\\n",
    "              index=False, chunksize=5000, dtype = outputdict)\n",
    "\n",
    "    log.info(\"Completed {} Import\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Imported: {} rows\".format(df.shape[0]))\n",
    "\n",
    "#%load_ext sql\n",
    "#%sql postgresql://jfreeley@localhost:5432/inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOB Permits Issued\n",
    "https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Violations/wvxf-dwi5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T11:58:23.223936",
     "start_time": "2016-11-19T11:58:23.145847"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "'borough':                                'object',\n",
    "'bin_num':                               'float64',\n",
    "'house_num':                              'object',\n",
    "'street_name':                            'object',\n",
    "'job_num':                               'float64',\n",
    "'job_doc._num':                          'float64',\n",
    "'job_type':                               'object',\n",
    "'self_cert':                              'object',\n",
    "'block':                                 'float64',\n",
    "'lot':                                    'object',\n",
    "'community_board':                        'object',\n",
    "'zip_code':                               'object',\n",
    "'bldg_type':                             'float64',\n",
    "'residential':                            'object',\n",
    "'special_district_1':                     'object',\n",
    "'special_district_2':                     'object',\n",
    "'work_type':                              'object',\n",
    "'permit_status':                          'object',\n",
    "'filing_status':                          'object',\n",
    "'permit_type':                            'object',\n",
    "'permit_sequence_num':                   'float64',\n",
    "'permit_subtype':                         'object',\n",
    "'oil_gas':                                'object',\n",
    "'site_fill':                              'object',\n",
    "'filing_date':                            'object',\n",
    "'issuance_date':                          'object',\n",
    "'expiration_date':                        'object',\n",
    "'job_start_date':                         'object',\n",
    "'permittees_first_name':                  'object',\n",
    "'permittees_last_name':                   'object',\n",
    "'permittees_business_name':               'object',\n",
    "'permittees_phone_num':                   'object',\n",
    "'permittees_license_type':                'object',\n",
    "'permittees_license_num':                 'object',\n",
    "'act_as_superintendent':                  'object',\n",
    "'permittees_other_title':                 'object',\n",
    "'hic_license':                            'object',\n",
    "'site_safety_mgrs_first_name':            'object',\n",
    "'site_safety_mgrs_last_name':             'object',\n",
    "'site_safety_mgr_business_name':          'object',\n",
    "'superintendent_first_and_last_name':     'object',\n",
    "'superintendent_business_name':           'object',\n",
    "'owners_business_type':                   'object',\n",
    "'non-profit':                             'object',\n",
    "'owners_business_name':                   'object',\n",
    "'owners_first_name':                      'object',\n",
    "'owners_last_name':                       'object',\n",
    "'owners_house_num':                       'object',\n",
    "'owners_house_street_name':               'object',\n",
    "'owners_house_city':                      'object',\n",
    "'owners_house_state':                     'object',\n",
    "'owners_house_zip_code':                  'object',\n",
    "'owners_phone_num':                       'object',\n",
    "'dobrundate':                             'object'}\n",
    "\n",
    "parse_dates = ['filing_date', 'issuance_date', 'expiration_date', 'job_start_date', 'dobrundate']\n",
    "\n",
    "keep_cols = [\n",
    "    'borough',\n",
    "    'bin_num',\n",
    "    'house_num',\n",
    "    'street_name',\n",
    "    'job_num',\n",
    "    'job_doc._num',\n",
    "    'job_type',\n",
    "    'block',\n",
    "    'lot',\n",
    "    'zip_code',\n",
    "    'bldg_type',\n",
    "    'residential',\n",
    "    'work_type',\n",
    "    'permit_status',\n",
    "    'filing_status',\n",
    "    'permit_type',\n",
    "    'filing_date',\n",
    "    'issuance_date',\n",
    "    'expiration_date',\n",
    "    'job_start_date',\n",
    "    'dobrundate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T12:49:26.381860",
     "start_time": "2016-11-19T12:11:16.931627"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value '83-09'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'ISSUED'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'A2'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'BRICK BY BRICK CONSTRUCTI'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'QUEENS'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'Y'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'DANIEL'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'YES'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'CARLOS'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'RAMIREZ'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n"
     ]
    }
   ],
   "source": [
    "parse_dates = None\n",
    "description = 'DOB Permits'\n",
    "input_csv_file = BASE_DIR+'DOB/Data Files/IssuedPermits/DOB_Permit_Issuance.csv'  \n",
    "output_pickle = BASE_DIR+'DOB/Data Files/IssuedPermits/df_dob_permit.pkl' \n",
    "input_pickle = BASE_DIR+'DOB/Data Files/IssuedPermits/df_dob_permit.pkl' \n",
    "sep_char = \",\"\n",
    "table_name = \"dob_permits\"\n",
    "load_pickle = False\n",
    "db_action = \"replace\"\n",
    "\n",
    "\n",
    "if load_pickle == True:\n",
    "    log.info(\"Flagged load of PICKLE: {} = True\".format(input_pickle))\n",
    "    with open(input_pickle, 'r') as picklefile:\n",
    "        log.info(\"Begin OPEN {} Pickle: {}\".format(input_pickle, datetime.datetime.now()))\n",
    "        log.info(\"Great we have a pickle file...Loading from {}\".format(input_pickle))\n",
    "        df = pickle.load(picklefile)\n",
    "else: \n",
    "    df = pd.read_csv(input_csv_file , sep=sep_char, dtype=dtype_dict, parse_dates=parse_dates)\n",
    "\n",
    "cols = [i.lower().replace(\" \",\"_\").replace(\"'\",\"\").replace(\"\\xe2\\x80\\x99\",\"\").replace(\"#\",\"num\").replace(\"&\",\"and\") for i in df.columns]\n",
    "df.columns = cols\n",
    "\n",
    "df = df[~df['borough'].str.contains(\"\\|\")]\n",
    "df['owners_business_name'].fillna(\"\",inplace=True)\n",
    "df = df[~df['owners_business_name'].str.contains(\"\\|\")]\n",
    "\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "df['issuance_date'] = pd.to_datetime(df['issuance_date'])\n",
    "df['expiration_date'] = pd.to_datetime(df['expiration_date'])\n",
    "df['job_start_date'] = pd.to_datetime(df['job_start_date'])\n",
    "df['dobrundate'] = pd.to_datetime(df['dobrundate'])\n",
    "\n",
    "with open(output_pickle, 'w') as picklefile:\n",
    "    log.info(\"Begin {} Pickle: {}\".format(description,datetime.datetime.now()))\n",
    "    pickle.dump(df, picklefile)\n",
    "\n",
    "outputdict = guess_sqlcol(df)        \n",
    "        \n",
    "if db_action == 'replace': \n",
    "        action = db_action \n",
    "else:\n",
    "        action = 'append'\n",
    "        \n",
    "df.to_sql(name=table_name, con=engine, if_exists = action,\\\n",
    "              index=False, chunksize=2500, dtype = outputdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-19T12:11:08.611453",
     "start_time": "2016-11-19T12:11:04.413853"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['owners_business_name'].fillna(\"\",inplace=True)\n",
    "df[~df['owners_business_name'].str.contains(\"\\|\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-20T13:34:52.730682",
     "start_time": "2016-11-20T13:34:52.653209"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col3</th>\n",
       "      <th>col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col3  col5\n",
       "0     1     1     1\n",
       "1     2     2     2\n",
       "2     3     3     3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"col1\":[1,2,3],\"col2\":[1,2,3],\"col3\":[1,2,3],\"col4\":[1,2,3],\"col5\":[1,2,3]})\n",
    "df\n",
    "keep = [\"col1\", \"col3\", \"col5\"]\n",
    "df = df[keep]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
