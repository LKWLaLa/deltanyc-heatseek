{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tables\n",
    "\n",
    "TO PREPARE:\n",
    " - The .csv files referenced below are STRAIGHT exports from the Socrata Files. \n",
    " - Each Socrata file is linked below. \n",
    "\n",
    " - The 311 data .csv import implies that you downloaded a monolithic 311 file from Socrata and then ran bash \"split\" on the file. \n",
    " - Once the split files were obtained, you then further processed them to `cat columns.311 [split_file] > [split_file]_c` to obtain properly \"headered\" split files. \n",
    "\n",
    "TO DO: \n",
    " - Please note that the guess_sql code above makes absurdly large varchar fields to account for large description fields in some data tables (specifically HPD Violations NOVDescription)\n",
    " - Need to clean up the field names for 311\n",
    "\n",
    "NOTES: \n",
    " - Far below is some random SQL SELECT statements\n",
    " - Far below are SQL statements for creating table indices\n",
    " - Questions: jpf321@gmail.com slack: jpfreeley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import desired libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:27:09.414586",
     "start_time": "2016-11-18T12:27:07.737621"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook will log to /root/db_import.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "BASE_DIR = '/root/'\n",
    "\n",
    "LOG_FILE = BASE_DIR+'db_import.log'\n",
    "\n",
    "logging.basicConfig(format= '[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    filename=LOG_FILE, \n",
    "    level=logging.INFO)\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "print \"This notebook will log to {}\".format(LOG_FILE)\n",
    "log.info(\"This notebook will log to {}\".format(LOG_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T13:40:58.696220",
     "start_time": "2016-11-16T13:40:58.691108"
    }
   },
   "source": [
    "# Initialize connection to AWS mySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:28:03.212706",
     "start_time": "2016-11-18T12:28:03.153883"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-30afad5ef900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmysql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MYSQL_USER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'mysql'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "user = os.environ['MYSQL_USER']\n",
    "host = os.environ['MYSQL_HOST']\n",
    "pw = os.environ['MYSQL_PASSWORD']\n",
    "db = os.environ['MYSQL_DATABASE']\n",
    "\n",
    "conn_str = \"mysql+mysqlconnector://{0}:{1}@{2}/{3}\".format(user, pw, host, db)\n",
    "engine = create_engine(conn_str, echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:28:05.962209",
     "start_time": "2016-11-18T12:28:05.847715"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def guess_sqlcol(dfparam):    \n",
    "\n",
    "## GUESS AT SQL COLUMN TYPES FROM DataFrame dtypes. \n",
    "    \n",
    "    dtypedict = {}\n",
    "    for i,j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.NVARCHAR(length=1024)}) ##big field length for HPD violations description\n",
    "\n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.Float(precision=20, asdecimal=True)}) ##big precision for LAT/LONG fields\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sqlalchemy.types.INT()})\n",
    "\n",
    "    return dtypedict\n",
    "\n",
    "\n",
    "def hpd_csv2sql(description, input_csv_file, sep_char, output_pickle,\\\n",
    "            table_name, dtype_dict, parse_dates, load_pickle, input_pickle, db_action):\n",
    "\n",
    "    log.info(\"Beginning {} Import {}\".format(description,datetime.datetime.now()))\n",
    "    \n",
    "    if load_pickle == True:\n",
    "        log.info(\"Flagged load of PICKLE: {} = True\".format(input_pickle))\n",
    "        \n",
    "        with open(input_pickle, 'r') as picklefile:\n",
    "            log.info(\"Begin OPEN {} Pickle: {}\".format(input_pickle, datetime.datetime.now()))\n",
    "            log.info(\"Great we have a pickle file...Loading from {}\".format(input_pickle))\n",
    "            df = pickle.load(picklefile)\n",
    "\n",
    "    else: \n",
    "        log.info(\"Reading CSV from {} .. This may take a while...\".format(input_csv_file))\n",
    "        \n",
    "        with open(input_csv_file, 'r') as input_csv: ## should just change to IF EXISTS rather than open()???\n",
    "            df = pd.read_csv(input_csv_file , sep=sep_char, dtype=dtype_dict, parse_dates=parse_dates)\n",
    "        \n",
    "        log.info(\"Why don't we save {} for next time\".format(output_pickle))\n",
    "        \n",
    "        with open(output_pickle, 'w') as picklefile:\n",
    "            log.info(\"Begin {} Pickle: {}\".format(description,datetime.datetime.now()))\n",
    "            pickle.dump(df, picklefile)\n",
    "\n",
    "    log.info(\"Let's now try to send it to the DB\")\n",
    "    outputdict = guess_sqlcol(df)  #Guess at SQL columns based on DF dtypes\n",
    "\n",
    "    log.info(\"Begin Upload {} SQL\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Let's see if we should replace or append our table ...\")\n",
    "\n",
    "    if db_action == 'replace': \n",
    "        \n",
    "        action = db_action \n",
    "\n",
    "    else:\n",
    "        \n",
    "        action = 'append'\n",
    "    \n",
    "    log.info(\"We're going with db_action = {}\".format(action))\n",
    "    log.info(\"Sending our df to {}\".format(table_name))\n",
    "    df.to_sql(name=table_name, con=engine, if_exists = action,\\\n",
    "              index=False, chunksize=5000, dtype = outputdict)\n",
    "\n",
    "    log.info(\"Completed {} Import\".format(description, datetime.datetime.now()))\n",
    "    log.info(\"Imported: {} rows\".format(df.shape[0]))\n",
    "\n",
    "#%load_ext sql\n",
    "#%sql postgresql://jfreeley@localhost:5432/inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Violations\n",
    "https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Violations/wvxf-dwi5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:17:22.590413",
     "start_time": "2016-11-18T11:53:17.259022"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vio_dtype_dict = {\n",
    "'ViolationID':                'int64',\n",
    "'BuildingID':                 'int64',\n",
    "'RegistrationID':             'int64',\n",
    "'BoroID':                     'int64',\n",
    "'Boro':                      'object',\n",
    "'HouseNumber':               'object',\n",
    "'LowHouseNumber':            'object',\n",
    "'HighHouseNumber':           'object',\n",
    "'StreetName':                'object',\n",
    "'StreetCode':                 'int64',\n",
    "'Zip':                      'float64',\n",
    "'Apartment':                 'object',\n",
    "'Story':                     'object',\n",
    "'Block':                      'int64',\n",
    "'Lot':                        'int64',\n",
    "'Class':                     'object',\n",
    "'InspectionDate':            'object',\n",
    "'ApprovedDate':              'object',\n",
    "'OriginalCertifyByDate':     'object',\n",
    "'OriginalCorrectByDate':     'object',\n",
    "'NewCertifyByDate':          'object',\n",
    "'NewCorrectByDate':          'object',\n",
    "'CertifiedDate':             'object',\n",
    "'OrderNumber':               'object',\n",
    "'NOVID':                    'float64',\n",
    "'NOVDescription':            'object',\n",
    "'NOVIssuedDate':             'object',\n",
    "'CurrentStatusID':            'int64',\n",
    "'CurrentStatus':             'object',\n",
    "'CurrentStatusDate':         'object'\n",
    "}    \n",
    "\n",
    "vio_parse_dates = ['InspectionDate',\n",
    "'ApprovedDate',\n",
    "'OriginalCertifyByDate',\n",
    "'OriginalCorrectByDate',\n",
    "'NewCertifyByDate',\n",
    "'NewCorrectByDate',\n",
    "'CertifiedDate',\n",
    "'NOVIssuedDate',\n",
    "'CurrentStatusDate']       \n",
    "    \n",
    "vio_description = \"HPD Violations\"\n",
    "vio_input_csv_file = BASE_DIR+'Violations/Housing_Maintenance_Code_Violations_trim.csv'\n",
    "vio_sep_char = \",\"\n",
    "vio_output_pickle = BASE_DIR+'Violations/df_violations2.pkl'\n",
    "vio_table_name = 'hpd_violations2'\n",
    "vio_load_pickle = True\n",
    "vio_input_pickle = BASE_DIR+'Violations/df_violations.pkl'\n",
    "vio_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "    \n",
    "hpd_csv2sql(\n",
    "            vio_description,\n",
    "            vio_input_csv_file, \n",
    "            vio_sep_char,\n",
    "            vio_output_pickle, \n",
    "            vio_table_name, \n",
    "            vio_dtype_dict, \n",
    "            vio_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            vio_input_pickle,\n",
    "            'replace' # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Buildings\n",
    "https://data.cityofnewyork.us/Housing-Development/Buildings-Subject-to-HPD-Jurisdiction/kj4p-ruqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:35:16.503497",
     "start_time": "2016-11-18T12:32:39.404390"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'OLD  LAW TENEMENT'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value '403'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value '10016'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value '401'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'Building'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'PVT'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'MANHATTAN'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'NOT AVAILABLE'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value 'HEREAFTER ERECTED CLASS A'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "/Applications/anaconda/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/sqltypes.py:219: SAWarning: Unicode type received non-unicode bind param value '1006'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n"
     ]
    }
   ],
   "source": [
    "bld_dtype_dict = {\n",
    "'BuildingID':              'int64',\n",
    "'BoroID':                  'int64',\n",
    "'Boro':                   'object',\n",
    "'HouseNumber':            'object',\n",
    "'LowHouseNumber':         'object',\n",
    "'HighHouseNumber':        'object',\n",
    "'StreetName':             'object',\n",
    "'Zip':                    'object',\n",
    "'Block':                   'int64',\n",
    "'Lot':                     'int64',\n",
    "'BIN':                   'float64',\n",
    "'CommunityBoard':          'int64',\n",
    "'CensusTract':           'float64',\n",
    "'ManagementProgram':      'object',\n",
    "'DoBBuildingClassID':    'float64',\n",
    "'DoBBuildingClass':       'object',\n",
    "'LegalStories':          'float64',\n",
    "'LegalClassA':           'float64',\n",
    "'LegalClassB':           'float64',\n",
    "'RegistrationID':          'int64',\n",
    "'LifeCycle':              'object',\n",
    "'RecordStatusID':          'int64',\n",
    "'RecordStatus':           'object'\n",
    "}\n",
    "\n",
    "bld_parse_dates = ''\n",
    "\n",
    "bld_description = \"HPD Buildings\"\n",
    "bld_input_csv_file = BASE_DIR+'Buildings/Buildings_Subject_to_HPD_Jurisdiction.csv'\n",
    "bld_sep_char = \",\"\n",
    "bld_output_pickle = BASE_DIR+'Buildings/df_buildings2.pkl'\n",
    "bld_table_name = 'hpd_buildings2'\n",
    "bld_load_pickle = True\n",
    "bld_input_pickle = BASE_DIR+'Buildings/df_buildings.pkl'\n",
    "bld_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "\n",
    "hpd_csv2sql(\n",
    "            bld_description,\n",
    "            bld_input_csv_file, \n",
    "            bld_sep_char,\n",
    "            bld_output_pickle, \n",
    "            bld_table_name, \n",
    "            bld_dtype_dict, \n",
    "            bld_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            bld_input_pickle,\n",
    "            'replace' # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Complaints\n",
    "https://data.cityofnewyork.us/Housing-Development/Housing-Maintenance-Code-Complaints/uwyv-629c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:43:42.941648",
     "start_time": "2016-11-18T12:39:23.859373"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Complaints Import 2016-11-18 12:39:23.886396\n"
     ]
    }
   ],
   "source": [
    "cmp_dtype_dict = {\n",
    "'ComplaintID':         'int64',\n",
    "'BuildingID':          'int64',\n",
    "'BoroughID':           'int64',\n",
    "'Borough':            'object',\n",
    "'HouseNumber':        'object',\n",
    "'StreetName':         'object',\n",
    "'Zip':               'float64',\n",
    "'Block':               'int64',\n",
    "'Lot':                 'int64',\n",
    "'Apartment':          'object',\n",
    "'CommunityBoard':      'int64',\n",
    "'ReceivedDate':       'object',\n",
    "'StatusID':            'int64',\n",
    "'Status':             'object',\n",
    "'StatusDate':         'object'\n",
    "}\n",
    "\n",
    "cmp_parse_dates = ['StatusDate','ReceivedDate']\n",
    "\n",
    "cmp_description = \"HPD Complaints\"\n",
    "cmp_input_csv_file = BASE_DIR+'Complaints/Housing_Maintenance_Code_Complaints.csv'\n",
    "cmp_sep_char = \",\"\n",
    "cmp_output_pickle = BASE_DIR+'Complaints/df_complaints2.pkl'\n",
    "cmp_table_name = 'hpd_complaints2'\n",
    "cmp_load_pickle = True\n",
    "cmp_input_pickle = BASE_DIR+'Complaints/df_complaints.pkl'\n",
    "cmp_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "\n",
    "hpd_csv2sql(\n",
    "            cmp_description,\n",
    "            cmp_input_csv_file, \n",
    "            cmp_sep_char,\n",
    "            cmp_output_pickle, \n",
    "            cmp_table_name, \n",
    "            cmp_dtype_dict, \n",
    "            cmp_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            cmp_input_pickle,\n",
    "            'replace' # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPD Complaint - Problems\n",
    "https://data.cityofnewyork.us/Housing-Development/Complaint-Problems/a2nx-4u46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:55:32.712387",
     "start_time": "2016-11-18T12:44:50.535593"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning ComplaintsProb Import 2016-11-18 12:44:50.559535\n"
     ]
    }
   ],
   "source": [
    "cpb_dtype_dict = {\n",
    "'ProblemID':             'int64',\n",
    "'ComplaintID':           'int64',\n",
    "'UnitTypeID':            'int64',\n",
    "'UnitType':             'object',\n",
    "'SpaceTypeID':           'int64',\n",
    "'SpaceType':            'object',\n",
    "'TypeID':                'int64',\n",
    "'Type':                 'object',\n",
    "'MajorCategoryID':       'int64',\n",
    "'MajorCategory':        'object',\n",
    "'MinorCategoryID':       'int64',\n",
    "'MinorCategory':        'object',\n",
    "'CodeID':                'int64',\n",
    "'Code':                 'object',\n",
    "'StatusID':              'int64',\n",
    "'Status':               'object',\n",
    "'StatusDate':           'object',\n",
    "'StatusDescription':    'object',\n",
    "}\n",
    "cpb_parse_dates = ['StatusDate']\n",
    "\n",
    "cpb_description = \"HPD ComplaintProblems\"\n",
    "cpb_input_csv_file = BASE_DIR+'Complaints/Complaint_Problems.csv'\n",
    "cpb_sep_char = \",\"\n",
    "cpb_output_pickle = BASE_DIR+'Complaints/df_prob2.pkl'\n",
    "cpb_table_name = 'hpd_complaints2'\n",
    "cpb_load_pickle = True\n",
    "cpb_input_pickle = BASE_DIR+'Complaints/df_prob.pkl'\n",
    "cpb_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "\n",
    "hpd_csv2sql(\n",
    "            cpb_description,\n",
    "            cpb_input_csv_file, \n",
    "            cpb_sep_char,\n",
    "            cpb_output_pickle, \n",
    "            cpb_table_name, \n",
    "            cpb_dtype_dict, \n",
    "            cpb_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            cpb_input_pickle,\n",
    "            'replace' # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registrations\n",
    "https://data.cityofnewyork.us/Housing-Development/Multiple-Dwelling-Registrations/tesw-yqqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T12:57:51.633897",
     "start_time": "2016-11-18T12:56:46.428640"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Registrations Import 2016-11-18 12:56:46.455010\n"
     ]
    }
   ],
   "source": [
    "reg_dtype_dict = {\n",
    "'RegistrationID':            'int64',\n",
    "'BuildingID':                'int64',\n",
    "'BoroID':                    'int64',\n",
    "'Boro':                     'object',\n",
    "'HouseNumber':              'object',\n",
    "'LowHouseNumber':           'object',\n",
    "'HighHouseNumber':          'object',\n",
    "'StreetName':               'object',\n",
    "'StreetCode':               'int64',\n",
    "'Zip':                     'float64',\n",
    "'Block':                     'int64',\n",
    "'Lot':                       'int64',\n",
    "'BIN':                     'float64',\n",
    "'CommunityBoard':            'int64',\n",
    "'LastRegistrationDate':     'object',\n",
    "'RegistrationEndDate':      'object'}\n",
    "\n",
    "reg_parse_dates = ['LastRegistrationDate', 'RegistrationEndDate']\n",
    "\n",
    "reg_description = \"HPD Registrations\"\n",
    "reg_input_csv_file = BASE_DIR+'Registrations/Multiple_Dwelling_Registrations.csv'\n",
    "reg_sep_char = \",\"\n",
    "reg_output_pickle = BASE_DIR+'Registrations/df_reg2.pkl'\n",
    "reg_table_name = 'hpd_registrations2'\n",
    "reg_load_pickle = True\n",
    "reg_input_pickle = BASE_DIR+'Registrations/df_reg.pkl'\n",
    "reg_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "\n",
    "\n",
    "hpd_csv2sql(\n",
    "            reg_description,\n",
    "            reg_input_csv_file, \n",
    "            reg_sep_char,\n",
    "            reg_output_pickle, \n",
    "            reg_table_name, \n",
    "            reg_dtype_dict, \n",
    "            reg_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            reg_input_pickle,\n",
    "            'replace' # DB ACTiON set as REPLACE (rather than APPEND)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration Contacts\n",
    "https://data.cityofnewyork.us/Housing-Development/Registration-Contacts/feu5-w2e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T13:15:39.968507",
     "start_time": "2016-11-18T13:15:39.888371"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/jfreeley/Desktop/HeatSeek/311/Data Files/2016_Jan1-Nov14/Registrations/df_regCon.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8b2a34fafbcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mrcn_input_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;34m'replace'\u001b[0m \u001b[0;31m# DB A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             )\n",
      "\u001b[0;32m<ipython-input-3-9159f15a8479>\u001b[0m in \u001b[0;36mhpd_csv2sql\u001b[0;34m(description, input_csv_file, sep_char, output_pickle, table_name, dtype_dict, parse_dates, load_pickle, input_pickle, db_action)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Flagged load of PICKLE: {} = True\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpicklefile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Begin OPEN {} Pickle: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Great we have a pickle file...Loading from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/jfreeley/Desktop/HeatSeek/311/Data Files/2016_Jan1-Nov14/Registrations/df_regCon.pkl'"
     ]
    }
   ],
   "source": [
    "rcn_dtype_dict = {\n",
    "'RegistrationContactID':     'int64',\n",
    "'RegistrationID':            'int64',\n",
    "'Type':                     'object',\n",
    "'ContactDescription':       'object',\n",
    "'CorporationName':          'object',\n",
    "'Title':                    'object',\n",
    "'FirstName':                'object',\n",
    "'MiddleInitial':            'object',\n",
    "'LastName':                 'object',\n",
    "'BusinessHouseNumber':      'object',\n",
    "'BusinessStreetName':       'object',\n",
    "'BusinessApartment':        'object',\n",
    "'BusinessCity':             'object',\n",
    "'BusinessState':            'object',\n",
    "'BusinessZip':              'object'\n",
    "    }\n",
    "\n",
    "rcn_parse_dates = ''\n",
    "\n",
    "rcn_description = \"HPD RegistrationsContacts\"\n",
    "rcn_input_csv_file = BASE_DIR+'Registrations/Registration_Contacts.csv'\n",
    "rcn_sep_char = \",\"\n",
    "rcn_output_pickle = BASE_DIR+'Registrations/df_regCon2.pkl'\n",
    "rcn_table_name = 'hpd_registrationContact2'\n",
    "rcn_load_pickle = True\n",
    "rcn_input_pickle = BASE_DIR+'Registrations/df_regCon.pkl'\n",
    "rcn_db_action = 'replace' ## if not = 'replace' then 'append' \n",
    "\n",
    "hpd_csv2sql(\n",
    "            rcn_description,\n",
    "            rcn_input_csv_file, \n",
    "            rcn_sep_char,\n",
    "            rcn_output_pickle, \n",
    "            rcn_table_name, \n",
    "            rcn_dtype_dict, \n",
    "            rcn_parse_dates,\n",
    "            True,     # ATTEMPT TO LOAD PICKLE FILE (specfified above as 'input_pickle')\n",
    "            rcn_input_pickle,\n",
    "            'replace' # DB A\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 311 Import\n",
    "https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T13:04:16.945100",
     "start_time": "2016-11-18T13:04:16.855439"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_311_col_dict = {'Unique Key':'int64',\n",
    "'Created Date':'object',\n",
    "'Closed Date':'object',\n",
    "'Agency':'object',\n",
    "'Agency Name':'object',\n",
    "'Complaint Type':'object',\n",
    "'Descriptor':'object',\n",
    "'Location Type':'object',\n",
    "'Incident Zip':'object',\n",
    "'Incident Address':'object',\n",
    "'Street Name':'object',\n",
    "'Cross Street 1':'object',\n",
    "'Cross Street 2':'object',\n",
    "'Intersection Street 1':'object',\n",
    "'Intersection Street 2':'object',\n",
    "'Address Type':'object',\n",
    "'City':'object',\n",
    "'Landmark':'object',\n",
    "'Facility Type':'object',\n",
    "'Status':'object',\n",
    "'Due Date':'object',\n",
    "'Resolution Description':'object',\n",
    "'Resolution Action Updated Date':'object',\n",
    "'Community Board':'object',\n",
    "'Borough':'object',\n",
    "'X Coordinate (State Plane)':'float64',\n",
    "'Y Coordinate (State Plane)':'float64',\n",
    "'Park Facility Name':'object',\n",
    "'Park Borough':'object',\n",
    "'School Name':'object',\n",
    "'School Number':'object',\n",
    "'School Region':'object',\n",
    "'School Code':'object',\n",
    "'School Phone Number':'object',\n",
    "'School Address':'object',\n",
    "'School City':'object',\n",
    "'School State':'object',\n",
    "'School Zip':'object',\n",
    "'School Not Found':'object',\n",
    "'School or Citywide Complaint':'float64',\n",
    "'Vehicle Type':'object',\n",
    "'Taxi Company Borough':'object',\n",
    "'Taxi Pick Up Location':'object',\n",
    "'Bridge Highway Name':'object',\n",
    "'Bridge Highway Direction':'object',\n",
    "'Road Ramp':'object',\n",
    "'Bridge Highway Segment':'object',\n",
    "'Garage Lot Name':'object',\n",
    "'Ferry Direction':'object',\n",
    "'Ferry Terminal Name':'object',\n",
    "'Latitude':'float64',\n",
    "'Longitude':'float64',\n",
    "'Location':'object'}\n",
    "\n",
    "parse_dates = ['Created Date','Closed Date','Due Date', 'Resolution Action Updated Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: each \"split\" of 250K rows takes about 15min on a macbook air laptop over wifi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-11-18T18:29:45.600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/jfreeley/Desktop/HeatSeek/311/Data Files/2016_Jan1-Nov14/'\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xaa_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'replace', index=False, chunksize=2500, dtype = outputdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T15:42:35.690566",
     "start_time": "2016-11-16T15:08:38.798811"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading SQL f\n",
      "Uploading SQL g\n"
     ]
    }
   ],
   "source": [
    "df_311 = pd.read_csv(BASE_DIR + 'xab_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xac_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xad_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xae_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xaf_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL i\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xag_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xah_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)\n",
    "\n",
    "df_311 = pd.read_csv(BASE_DIR + 'xai_c', sep=',',encoding='utf8',\\\n",
    "                     infer_datetime_format=True, parse_dates=parse_dates, dtype=call_311_col_dict)\n",
    "outputdict = guess_sqlcol(df_311)  \n",
    "print \"Uploading SQL\" \n",
    "df_311.to_sql(name='call_311', con=engine, if_exists = 'append', index=False, chunksize=2500, dtype = outputdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T22:57:43.075424",
     "start_time": "2016-11-16T22:57:43.049995"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Large 311 Select with ages\n",
    "#\n",
    "# SELECT \n",
    "# `Unique Key`,\n",
    "# `Created Date`, \n",
    "# `Closed Date`,\n",
    "# timestampdiff(day,`Created Date`,`Closed Date`) as AgeDays,\n",
    "# timestampdiff(hour,`Created Date`,`Closed Date`) as AgeHr,\n",
    "# `Agency`,\n",
    "#  `Complaint Type`\n",
    "# ,`Descriptor`,\n",
    "# `Location Type`,\n",
    "#  `Incident Zip`, \n",
    "# `Incident Address`,\n",
    "# `Facility Type`,\n",
    "#  `Status`,\n",
    "# `Due Date`,\n",
    "# `Borough`,\n",
    "#  `Resolution Description`,\n",
    "# `Resolution Action Updated Date`,\n",
    "# `Latitude`,\n",
    "#  `Longitude` \n",
    "# FROM `call_311` \n",
    "# WHERE Agency = \"HPD\" and `Complaint Type` = \"HEAT/HOT WATER\" and `Status` != \"Closed\"\n",
    "\n",
    "\n",
    "\n",
    "# COUNT OF HEAT/HW with locations\n",
    "#\n",
    "# SELECT * \n",
    "# FROM (\n",
    "# SELECT  `Incident Address` ,  `Borough` ,  `Latitude` ,  `Longitude` , COUNT(  `Unique Key` ) AS count, AVG( timestampdiff(\n",
    "# DAY ,  `Created Date` ,  `Closed Date`\n",
    "# ) ) AS average_day_age\n",
    "# FROM call_311\n",
    "# WHERE Agency =  \"HPD\"\n",
    "# AND  `Complaint Type` =  \"HEAT/HOT WATER\"\n",
    "# AND  `Status` =  'Closed'\n",
    "# GROUP BY  `Incident Address`\n",
    "# ) AS count_table\n",
    "# ORDER BY average_day_age DESC\n",
    "\n",
    "\n",
    "#\n",
    "#SELECT TABLE_ROWS, TABLE NAME\n",
    "#      FROM INFORMATION_SCHEMA.TABLES \n",
    "#      WHERE TABLE_SCHEMA = 'heatseak' AND\n",
    "#         TABLE_NAME NOT LIKE '%pma_%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## hpd_buildings indexes\n",
    "# ALTER TABLE `hpd_buildings` ADD PRIMARY KEY(`BuildingID`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`BoroID`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`RecordStatus`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`BIN`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`Lot`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`Block`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`StreetName`);\n",
    "# ALTER TABLE `hpd_buildings` ADD INDEX(`HouseNumber`);\n",
    "\n",
    "# ## hpd_complaints indexes\n",
    "# ALTER TABLE `hpd_complaints` ADD PRIMARY KEY(`ComplaintID`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`StatusDate`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`Status`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`Lot`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`Block`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`StreetName`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`HouseNumber`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`BoroughID`);\n",
    "# ALTER TABLE `hpd_complaints` ADD INDEX(`BuildingID`);\n",
    "\n",
    "# ## hpd_complaint_problem indexes\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD PRIMARY KEY(`ProblemID`);\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD INDEX(`ComplaintID`);\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD INDEX(`MajorCategory`);\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD INDEX(`MinorCategory`);\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD INDEX(`Status`);\n",
    "# ALTER TABLE `hpd_complaint_problem` ADD INDEX(`StatusDate`);\n",
    "\n",
    "# ## hpd_registration indexes\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`RegistrationID`); #1062 - Duplicate entry '913236' for key 'PRIMARY'\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`BuildingID`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`BoroID`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`HouseNumber`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`StreetName`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`Block`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`Lot`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`BIN`);\n",
    "# ALTER TABLE `hpd_registration` ADD INDEX(`RegistrationEndDate`);\n",
    "\n",
    "# ## hpd_registrationContact indexes\n",
    "# ALTER TABLE `hpd_registrationContact` ADD INDEX(`RegistrationContactID`); ##1062 - Duplicate entry '91323603' for key 'PRIMARY'\n",
    "# ALTER TABLE `hpd_registrationContact` ADD INDEX(`RegistrationID`);\n",
    "\n",
    "# ## call_311 indexes\n",
    "# ALTER TABLE `call_311` ADD PRIMARY KEY(`Unique Key`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Created Date`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Agency`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Complaint Type`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Descriptor`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Incident Address`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Status`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Latitude`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Longitude`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Resolution Description`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Resolution Action Updated Date`);\n",
    "# ALTER TABLE `call_311` ADD INDEX(`Borough`);\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
